---
title: "STSCI 5740 Final Project"
output: html_document
date: "2023-04-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)
library(randomForest)
library(leaps)
library(glmnet)

data <- read_csv("redone_data_analysis6.csv")

```

```{r}
# LASSO OR RIDGE REGRESSION WITH ALL PREDICTORS (lasso = good for sparse matrix with lots of 0's and high dimensional/lots of columns b/c of dummy variables)
# WITH TRAIN/TEST SPLIT


data$normalized_range_to <- (data$Salary.Range.To - mean(data$Salary.Range.To)) /sd(data$Salary.Range.To)

data$normalized_range_from <- (data$Salary.Range.From - mean(data$Salary.Range.From)) /sd(data$Salary.Range.From)

cleaned_data <- data[,!names(data) %in% c("Salary.Range.From", "Salary.Range.To")] %>% na.omit



data_X <- model.matrix(normalized_range_from ~ ., data=cleaned_data)
data_Y = cleaned_data$normalized_range_from

# TRAIN TEST SPLIT
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(data_X), replace=TRUE, prob=c(0.7,0.3))
train_X  <- data_X[sample, ]
test_X   <- data_X[!sample, ]

train_Y = data_Y[sample]
test_Y = data_Y[!sample]


cv.out=cv.glmnet(train_X,train_Y,alpha=1, standardize = TRUE) #10 fold cross validation
bestlam=cv.out$lambda.min
bestlam
lasso.mod=glmnet(train_X,train_Y,alpha=1,lambda=bestlam, standardize = TRUE)
# lasso.coef=coef(lasso.mod)[,1]
# lasso.coef[lasso.coef!=0]
# lasso.coef[lasso.coef==0]

y_predicted <- predict(lasso.mod, s = bestlam, newx = test_X)




mse = mean((y_predicted - test_Y)^2)
print(mse)


# for calculating R-squared value https://www.statology.org/lasso-regression-in-r/
#find SST and SSE
sst <- sum((test_Y - mean(test_Y))^2)
sse <- sum((y_predicted - test_Y)^2)

#find R-Squared
r_2 <- 1 - sse/sst
r_2

plot.new()
plot(test_Y, y_predicted)
abline(a = 0, b = 1)

```

